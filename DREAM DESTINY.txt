
Conversation opened. 1 unread message.

Skip to content
Using Gmail with screen readers
1 of 2,537
Dream Destiny
Inbox

Nathan Michel <foolishnessenvy@gmail.com>
4:26 PM (1 minute ago)
to me

Project Helios: A Technical Blueprint for an Autonomous Research Synthesis Agent
Part 1: The Vision and Strategic Architecture
This document provides a comprehensive technical blueprint for "Project Helios," an autonomous research synthesis system. The project's architecture, economic modeling, and implementation phases are designed to be executed using the Anthropic Claude API.
A. The Helios Vision: An Agent that Finds the Unknown Unknowns
The objective of Project Helios is not to build another summarization tool. The challenge for modern researchers, such as graduate students navigating "100+ updates in my RSS feed every week," is not a scarcity of information but a failure of synthesis.
The "dream" articulated here is an Autonomous Research Synthesis Agent. Its primary function is to transcend simple information retrieval—what science already knows —and to identify what it does not. The agent's core purpose is to discover "unexplored or unresolved area[s]" of research, commonly known as "research gaps". The ultimate goal for Helios is to process a body of literature and suggest the next experiment, the next hypothesis, or the next paper.
This project aims to construct a personalized, economically viable version of the ambitious, industrial-scale platforms described in contemporary AI research, such as OpenAI's "Deep Research" agent  and the comprehensive data analysis tools provided by platforms like Elicit.
B. The Agentic Framework: Designing the Helios Research Team
A complex goal requires a sophisticated approach. A single, monolithic prompt to a large language model (LLM) is both unreliable and financially inefficient for this task. The current state-of-the-art for AI-driven analysis, as seen in NVIDIA's Nemotron, relies on "specialized, intelligent agents" that can "see, retrieve, generate and reason" independently. Similarly, open-source efforts like Tongyi's "DeepResearch" are explicitly "Autonomous Agent[s]," not simple chatbots.
Therefore, the architecture of Helios must be a collaborative multi-agent system, where each agent performs a distinct cognitive task. This "chain-of-thought" process, separating the tasks of finding, filtering, reading, and synthesizing, is the key to both high-quality output and, as detailed in Part 2, a cost-effective system.
The Helios agent team consists of five specialized roles:
* The Librarian (Ingestion Agent): A non-LLM utility agent that queries external academic APIs (e.g., arXiv, PubMed) to fetch metadata for new publications based on a user's topic of interest.
* The Triage (Filtering Agent): A low-cost, high-speed LLM agent. Its sole function is "prefiltering"  by reading only the title and abstract to determine if a paper is relevant enough to merit a full, expensive analysis.
* The Analyst (Data Extraction Agent): A mid-tier, "workhorse" LLM agent. This agent performs a deep read of the full-text papers approved by the Triage agent. Its goal is to "automate... data extraction"  and "extract text and tables"  into a structured JSON format.
* The Indexer (Memory Agent): A non-LLM utility agent. It takes the structured JSON output from the Analyst, chunks and embeds it, and stores it in a local vector database for long-term memory and retrieval.
* The Synthesist (Insight Agent): The "executive" agent. This is the most intelligent and computationally expensive agent. It reviews all the Analyst's reports retrieved from the Indexer to "generate... literature reviews" , identify "emerging trends" , and, most importantly, "synthesize knowledge... [to] create new knowledge"  by identifying the research gaps.
C. System Architecture Blueprint: The Ingest-Process-Synthesize-Present Loop
The flow of data and analysis in the Helios system follows a continuous, six-step loop:
* Ingest: The user defines a research topic. The Librarian agent queries the arxiv  and PubMed  APIs, gathering a list of 100 recent paper IDs, titles, abstracts, and PDF URLs.
* Process (Triage): The abstracts of all 100 papers are passed to the Triage agent (running on the low-cost Claude Haiku model). The agent outputs a simple list of "relevant" or "irrelevant." Irrelevant papers are discarded, saving significant cost.
* Process (Analyze): The full-text PDF (or source) for the, for example, 20 "relevant" papers is downloaded. The text content is fed to the Analyst agent (running on the balanced Claude Sonnet model). This agent outputs 20 structured JSON analysis reports.
* Process (Memory): The Indexer agent (using chromadb ) embeds and stores these 20 JSON reports in its local vector database.
* Synthesize: The user submits a synthesis query (e.g., "Find the main gaps in our AI-driven drug discovery library"). The Indexer retrieves the most relevant JSON reports. These reports are "stuffed" into the context window of the Synthesist agent (running on the high-intelligence Claude Opus or Sonnet 4.5 model).
* Present: The Synthesist agent's final, synthesized analysis (a Markdown-formatted research brief) is presented to the user in a simple web interface (built with streamlit ).
D. Core Technology Stack (The Claude Code Shopping List)
The following components are required to build the Helios prototype:
* Core Logic: Python (version 3.8+ is recommended for the latest SDK support).
* AI Engine: The official Anthropic Python SDK (pip install anthropic).
* Data Ingestion:
   * arxiv library (pip install arxiv).
   * Biopython (pip install biopython) for PubMed's Entrez utilities.
* Memory/Database: ChromaDB (pip install chromadb).
* User Interface: Streamlit (pip install streamlit).
* Environment: p[span_31](start_span)[span_31](end_span)ython-dotenv (pip install python-dotenv) for secure API key management.
Part 2: Economic Viability: Engineering for a $1,000 Budget
A $1,000 API credit is a substantial resource, but it is finite. The project's architecture must treat this budget as a primary design constraint. A naive approach would exhaust these funds with no viable product. This section details a "Token-Miser" architecture designed for maximum longevity and value.
A. The Token-Mior Architecture: A-Priori Cost Modeling
The Anthropic API provides a spectrum of models, each with a different cost-performance profile. The cost differences are not trivial; they are structural.
* Claude Haiku (e.g., 3, 4.5): The speed and cost leader. Haiku 3 costs $0.25 per million input tokens and $1.25 per million output tokens. Haiku 4.5 costs $1.00/$5.00 per 1M input/output tokens.
* Claude Sonnet (e.g., 3.5, 4.5): The balanced "workhorse" model. Claude 3.5 Sonnet and 4.5 Sonnet cost $3.00 per million input tokens and $15.00 per million output tokens.
* Claude Opus (e.g., 4, 4.1): The "powerhouse" model for frontier intelligence. Claude 4.1 Opus costs $15.00 per million input tokens and $75.00 per million output tokens.
Comparing the input cost of Opus 4.1 ($15/MTok) to Haiku 3 ($0.25/MTok) reveals a 60x cost difference for the same task. Using the Opus model for a simple task like abstract filtering would be financially irresponsible.
This economic reality is the primary justification for the multi-agent architecture described in Part 1. By "cascading" tasks to the cheapest effective model, the system's financial burn rate can be dramatically reduced.
The Helios Model Selection Strategy:
* Triage Agent: Must use Claude Haiku 4.5. This is a high-volume, low-value task. Cost-per-token is the only metric that matters.
* Analyst Agent: Must use Claude Sonnet 4.5. This task requires a high-quality, reliable model that can follow complex JSON instructions. Sonnet provides the "best balance of intelligence, speed, and cost" for this "workhorse" task.
* Synthesist Agent: Should use Claude Sonnet 4.5 for most syntheses. The system will reserve Claude Opus 4.1 for only the most critical, final "executive review" reports where maximum intelligence is required.
B. Financial Modeling: The Helios Project Budget
To make the $1,000 credit tangible, a "cost-per-research-cycle" model is necessary. The following table models the cost for a single "Helios cycle": ingesting 100 papers from a query.
Assumptions for this model:
* 1 research cycle = 100 papers fetched.
* Average abstract: 500 tokens.
* Triage Rate: 20% (20 of 100 papers are deemed relevant for deep analysis).
* Average full-text paper: 8,000 tokens.
* Average Analyst JSON output: 1,500 tokens.
* Average Synthesist report output: 3,000 tokens.
* All costs are based on standard, non-cache pricing.
Table: Helios Project Token Budget (Cost Per 100 Papers Processed)
| Agent | Model Used | Unit (Workload) | Input Tokens | Output Tokens | Input Cost (USD) | Output Cost (USD) | Total Cost (USD) |
|---|---|---|---|---|---|---|---|
| Triage | Claude Haiku 4.5 ($1/$5 per MTok) | 100 abstracts | 50,000 | 5,000 | $0.05 | $0.025 | $0.075 |
| Analyst | Claude Sonnet 4.5 ($3/$15 per MTok) | 20 full papers | 160,000  | 30,000 | $0.48 | $0.45 | $0.93 |
| Synthesist | Claude Opus 4.1 ($15/$75 per MTok) | 1 synthesis report | 30,000 | 3,000 | $0.45 | $0.225 | $0.675 |
| Total |  | Per 100-Paper Cycle | 240,000 | 38,000 | $0.98 | $0.70 | $1.68 |
This financial model demonstrates that the Helios "dream" is economically viable. At a cost of $1.68 per 100-paper cycle, the $1,000 credit can fund approximately 595 research cycles. This would allow the system to ingest, analyze, and synthesize over 59,000 academic papers.
C. Advanced Cost-Saving Mechanisms: Caching and Batching
The $1,000 budget can be stretched even further by architecting the system to use Anthropic's advanced billing features. These are not automatic and must be explicitly coded into the solution.
* Prompt Caching:
   Anthropic's pricing tables reveal a massive discount for cached prompts. For Claude Sonnet 4.5, the "Base Input Tokens" cost is $3.00 per million tokens. However, the "Cache Hits & Refreshes" cost is only $0.30 per million tokens. This is a 90% cost reduction on input.
   The Analyst agent uses the exact same large system prompt (the JSON instructions) for every paper it analyzes. By enabling prompt caching in the API call, the 2nd through 20th paper in a cycle will treat this system prompt as a "cache hit," drastically reducing the input cost for the most expensive part of the workflow.
* Batch Processing:
   The documentation also mentions "50% cost savings with batch processing"  via the "Message Batches API". This is perfectly suited for the Triage agent. Instead of making 100 individual, high-latency API calls for the 100 abstracts, the system's logic must be written to aggregate all 100 abstracts first, format them into a single batch request, and submit them as one job. This will cut the cost of the triage phase in half.
D. Operational Constraints: Limits and Alerts
The API credit is governed by operational rules that must be respected.
* API Rate Limits: A new account will be in a "Usage Tier" with limits on requests-per-minute and tokens-per-minute. Crucially, the documentation warns that "a sharp increase in usage" can trigger a 429 error, even if under the token limit. The system must therefore be "polite." The code must "ramp up your traffic gradually"  by including small delays (e.g., time.sleep(1)) in its processing loops.
* Billing Alerts: A $1,000 credit can be burned surprisingly fast by a runaway script. The project must "Set up billing alerts immediately" in the Anthropic console. Furthermore, a "stop, if costs too much" failsafe  must be coded into the Helios application itself. This will be a simple "cost-tracker" class that reads the usage object from every API response , aggregates the cost, and halts all execution if a user-defined budget (e.g., $100) is exceeded.
Part 3: Phase 1 Implementation: The Ingestion and Memory Engine
This section provides the specific implementation details for the "Librarian" (data ingestion) and "Indexer" (data memory) agents.
A. Module 1: The Librarian Agent (Data Ingestion)
The Librarian's job is to populate the system with raw material. This requires interfacing with open academic APIs.
1. arXiv API Access
* Tool: The arxiv Python wrapper.
* Implementation: The arXiv API is a free, open resource, and its terms of use must be respected. A naive script that rapidly scrapes the site will be rate-limited or banned. The arxiv library provides a "polite client" for this purpose. The implementation must instantiate a Client object with conservative settings for pagination and retries.
* Prompt for Claude:
   > "Using the arxiv Python library , write a Python function fetch_arxiv_papers(query,[span_43](start_span)[span_43](end_span)[span_46](start_span)[span_46](end_span) max_results=100). This function must:
   >  * Instantiate a polite client: client = arxiv.Client(page_size=100, delay[span_22](start_span)[span_22](end_span)_seconds=5.0, num_retries=3).
   >  * Use arxiv.Search to search for the user's query, sorting by RELEVANCE or SUBMITTED_DATE.
   >  * Use client.results() to get the generator.
   >  * Iterate through the results and return a list of Python objects, each containing result.entry_id, result.title, result.summary, and result.pdf_url (from result.links).
   >  * Include exception handling and print status messages."
   >
2. PubMed API Access
* Tool: Biopython's Bio.Entrez module.
* Implementation: The NCBI E-utilities (which power PubMed and PMC) are designed for bulk access but are more complex. The key to downloading large result sets is to use the E-utilities' history server. A single Entrez.esearch call is made with usehistory="y". This stores the search results on NCBI's servers and returns a WebEnv and QueryKey. The system can then use Entrez.efetch in polite, small batches (e.g., 100 at a time) using these keys, which avoids "flooding the servers"  and overcomes the limitations on concurrent bulk extraction.
* Prompt for Claude:
   > "Using Biopython's Bio.Entrez module, write a function fetch_pubmed_papers(query, max_results=100). This function must:
   >  * Set Entrez.email to a user-provided email, as required by NCBI.
   >  * Use Entrez.esearch with db="pmc", term=query, retmax=max_results, and usehistory="y".
   >  * Extract the WebEnv and QueryKey from the esearch results.
   >  * In a loop (in case max_results is large), use En[span_51](start_span)[span_51](end_span)trez.efetch in batches (e.g., retmax=100) using the WebEnv and QueryKey to retrieve the article data.
   >  * Parse the returned XML  to extract the PMCID, ArticleTitle, and AbstractText for each paper.
   >  * Return a list of paper objects, similar to the arXiv function."
   >
B. Module 2: The Indexer Agent (Local Knowledge Base)
* Tool: ChromaDB.
* Implementation: This project requires a simple, beginner-friendly vector store for the Retrieval-Augmented Generation (RAG) pattern. While FAISS is a powerful library for similarity search , ChromaDB is a full-fledged vector database "tailored for AI applications". Its "Simple API"  and ability to store metadata alongside the embedded documents make it the superior choice for this project. The implementation will use ChromaDB's in-memory storage for rapid prototyping.
* Prompt for Claude:
   > "Using the chromadb Python library, write a VectorMemory class. This class must:
   >  * In its __init__, initialize an in-memory client: self.client = chromadb.Client().
   >  * Create or get a collection: self.collection = self.client.get_or_create_collection(name="helios_research").
   >  * Have an add_papers(paper_analyses) method. This method will take the list of JSON outputs from the Analyst agent.
   >  * It must add each analysis to Chroma. It should use the paper's entry_id (e.g., '2409.18812') as the id, the full JSON analysis as a string as the document to be embedded, and a dictionary of metadata (e.g., {"title": paper_title, "source": "arxiv"}) for filtering."
   >  * Have a query_memory(query_text, n_results=10) method that embeds the query_text and returns the n_results most similar documents (which are the JSON analysis strings) from the collection."
   >
Part 4: Phase 2 Implementation: The Helios Synthesis Core
This phase constructs the "brain" of the project: the Analyst and Synthesist agents that perform the analysis.
A. Module 3: The Analyst and Synthesist Agents
* Tool: The official anthropic Python SDK.
* Implementation: The core logic will be built using the official anthropic Python library. It is critical to use the modern Messages API (client.messages.create)  which accepts a list of messages with role and content. This is essential for agentic behavior and system prompts. The older, deprecated completions.create API, which uses HUMAN_PROMPT , must be avoided.
* Prompt for Claude (Base Client):
   > "Write a Python script helios_client.py. This script must:
   >  * Import os, anthropic, and json.
   >  * Load the ANTHROPIC_API_KEY from environment variables.
   >  * Instantiate the main client: client = anthropic.Anthropic().
   >  * Define a function run_helios_agent(system_prompt, user_message, model_name, max_tokens=4096, use_cache=False).
   >  * Inside this function, build the headers dictionary. If use_cache=True, add {"anthropic-cache-control": "max-age=86400"} to enable 90% cost savings.
   >  * The function will call client.messages.create(), passing model=model_name (e.g., 'claude-3-5-sonnet-20240620' ), max_tokens=max_tokens, system=system_prompt, messages=[{"role": "user", "content": user_message}], and the headers.
   >  * It must extract the text from the response's content block and return it.
   >  * Include a simple cost-tracking mechanism that reads the usage block from the response  and logs the token consumption."
   >
B. Prompt Engineering for Synthesis (The Analyst Agent)
The Analyst agent's purpose is to automate the "data extraction" phase of a systematic review. To be useful, its output must be structured and reliable.
* Prompt for Claude (Analyst System Prompt):
   > "Here is the system prompt for our Analyst agent. Store this as a constant named ANALYST_SYSTEM_PROMPT.
   > You are an expert, high-speed research analyst specializing in biomedical and computational science. Your task is to perform the data extraction step of a systematic review. You will be given the full text of a single academic paper. You must read the *entire* paper and return a JSON object (and *only* a JSON object) with the following structure. Do not include any preamble, apologies, or conversational text.
   >
   > {
   >   "paper_title": "The paper's full title",
   >   "summary_of_core_argument": "A one-paragraph summary of the paper's core thesis or argument.",
   >   "key_findings": [
   >     "A list of the 3-5 most important, concrete findings.",
   >     "Each finding should be a direct, verifiable claim from the paper."
   >   ],
   >   "methodology": "A brief description of the methodology used (e.g., 'RCT', 'computational model', 'systematic review').",
   >   "methodology_critique": "A brief, critical analysis of any strengths or weaknesses of the methodology mentioned by the authors or implied by the text.",
   >   "author_implied_future_work": "Any direct statements or strong implications from the 'Discussion' or 'Conclusion' sections about what future research is needed.",
   >   "potential_gap_identification": "Your own expert analysis of any 'open questions' [span_99](start_span)[span_99](end_span) or 'unresolved areas' this paper reveals. What does this paper *not* answer?"
   > }
   > ```"
   >
   >
C. Prompt Engineering for Gap Analysis (The Synthesist Agent)
This is the final step: synthesis. This agent's prompt is designed to take a collection of Analyst reports and produce a "research brief"  that identifies "conceptual and methodology gaps" —the project's core goal.
* Prompt for Claude (Synthesist System Prompt):
   > "Here is the system prompt for our Synthesist agent. Store this as a constant named SYNTHESIST_SYSTEM_PROMPT.
   > You are a world-class scientific synthesizer and strategist, a [span_5](start_span)[span_5](end_span)"giant" on whose shoulders others stand. You will be given a user's research query and a list of {N} JSON-formatted 'Analyst reports' from relevant papers.
   >
   > Your task is NOT to summarize each paper one-by-one. Your task is to synthesize *across* all {N} reports to generate a high-level "Research Brief".
   >
   > Your report must have three sections:
   >
   >
   >
> 1.  EMERGING TRENDS: What are the common themes, methodologies, or findings across these papers? What is the current "state of the art" as defined by this body of literature?
>
> 2.  AREAS OF CONFLICT OR DISAGREEMENT: Where do these papers disagree? Are there conflicting findings or different methodological approaches that lead to different results?
>
> 3.  THE IDENTIFIED RESEARCH GAP: This is the most important section. Based on the "author_implied_future_work" and "potential_gap_identification" fields from the analyst reports, and your own synthesis of the trends, what is the 'unexplored or unresolved area'? What is the 'open question'  that a new researcher should tackle? Provide a clear, actionable research question that represents a high-impact gap in the literature.
> ```"
Part 5: Phase 3 Implementation: The Research Dashboard
An agent's findings are only useful if they are accessible. This phase details the construction of a simple, interactive user interface.
A. Module 4: The Helios User Interface
* Tool: Streamlit.
* Implementation: The choice of UI framework is critical. Flask is a powerful backend framework, but it is "highly customizable"  precisely because it requires building "completely from scratch"  and mastering separate "frontend development". Streamlit, by contrast, is the ideal tool for this project. It is "useful if you want to get a dashboard up and running quickly" , is designed for data-heavy applications, and "Anyone who is familiar with Python can use [it]". This allows the project to focus on the AI logic, not on HTML, CSS, and JavaScript.
* Prompt for Claude (UI Code):
   > "Write the main Streamlit application script, app.py. This script must:
   >  * Import streamlit as st, as well as the functions from our librarian.py, vector_memory.py, and helios_client.py modules.
   >  * Set the page title: st.set_page_config(page_title="Project Helios").
   >  * Display a title: st.title("Project Helios: Autonomous Research Agent").
   >  * Create a st.sidebar for all user inputs.
   >  * The sidebar must contain:
   >    * st.text_input for the arxiv_query (e.g., "cat:cs.AI AND all:synthesis" ).
   >    * st.text_input for the pubmed_query (e.g., "systematic review[ti] AND llm[tiab]").
   >    * st.slider for max_papers (default 50).
   >    * st.button("Run Ingest & Analysis").
   >  * The main page area must contain:
   >    * st.text_input for the synthesis_query (e.g., "What are the main gaps in this literature?").
   >    * st.button("Generate Synthesis Report").
   >    * A placeholder for status messages: status_area = st.empty().
   >  * Define the application logic:
   >    * If the "Run Ingest" button is clicked:
   >      * Use with st.spinner("Running Librarian..."): to show progress.
   >      * Call fetch_arxiv_papers() and fetch_pubmed_papers().
   >      * Use with st.spinner("Running Triage (Haiku)..."): and with st.spinner("Running Analysis (Sonnet)..."): to show progress for each step.
   >      * Loop through papers, call the Triage and Analyst agents.
   >      * Call VectorMemory.add_papers() to save the results.
   >      * Show st.success("Ingestion complete! Ready for synthesis.").
   >    * If the "Generate Synthesis" button is clicked:
   >      * Use with st.spinner("Running Synthesist (Opus)..."):.
   >      * Call VectorMemory.query_memory(synthesis_query) to get relevant analyses.
   >      * Call the Synthesist agent (using the SYNTHESIST_SYSTEM_PROMPT).
   >      * Display the final report: st.markdown(synthesis_report)."
   >
Part 6: The Path Forward: From Prototype to Platform
The $1,000 API credit is a bootstrap, not a permanent solution. This final section provides a strategic path for expanding Project Helios beyond the initial prototype.
A. Migrating to Open-Source Models
The true intellectual property of Project Helios is not its reliance on the Claude API; it is the agentic logic and the curated prompts. Once this workflow is perfected using the API credits, the entire system can be "ported" to a self-hosted, open-source infrastructure.
The research shows a "deep commitment to open source" from major players like NVIDIA, which is contributing models like Nemotron to Hugging Face. Fully open-source web agents like Tongyi DeepResearch are also emerging , alongside other open-source machine learning platforms.
The migration path is straightforward:
* Download a powerful open-source model (e.g., from NVIDIA on Hugging Face ).
* Run this model locally using a free inference server (like Ollama or vLLM).
* Modify the helios_cli[span_14](start_span)[span_14](end_span)ent.py script to change the base_url of the API client to point to the local endpoint (http://localhost:11434) instead of api.anthropic.com.
The Analyst and Synthesist prompts remain 99% the same. The "dream" is now self-hosted and, aside from electricity, free to run indefinitely.
B. Future Vision: From Gaps to Graphs
The current Helios prototype identifies research gaps and presents them as a text report. The next-generation version should visualize them.
The research highlights innovative tools like Litmaps  and Open Knowledge Maps  that provide a "bird's-eye view" of literature by creating "knowledge maps." These tools visualize the connections between papers, helping researchers see the field's structure.
A future version of Helios could have its Analyst agent extract not just gaps, but also the paper's key citations. This citation data could then be used to generate a "Litmap"  or "knowledge map"  within the Streamlit dashboard, visually plotting the papers and highlighting the "unexplored" territory as a literal "gap" in the graph. This transforms Helios from a simple report-generator into a true mapmaker for scientific discovery. 
Please ask me any questions you have.

 
